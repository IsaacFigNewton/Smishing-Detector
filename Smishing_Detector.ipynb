{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMogRP28RGVpvQxDLA1Rl3O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsaacFigNewton/Smishing-Detector/blob/sklearn-approach/Smishing_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and config"
      ],
      "metadata": {
        "id": "KIzPua5nAUIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk as nlp"
      ],
      "metadata": {
        "id": "EPrEblmbASZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasetSize = 1\n",
        "minSusLen = 500\n",
        "\n",
        "charNGrams = range(1, 6)"
      ],
      "metadata": {
        "id": "99TY1c78AeUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important Functions"
      ],
      "metadata": {
        "id": "WVP9QkoYAWvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "Rf6ccobjAk4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# takes in a pd.Series, pd.DataFrame and returns an np.array\n",
        "def knn_predict_spam(document, corpus):\n",
        "    # clean the corpus\n",
        "    corpus = corpus.dropna(axis=0)\n",
        "\n",
        "    # extract the classifications and email bodies\n",
        "    classifications = np.array(corpus[\"class\"])\n",
        "    corpus = corpus[\"text\"]\n",
        "\n",
        "    # print(document)\n",
        "\n",
        "    # create and fit the vectorizer\n",
        "    vec = TfidfVectorizer(norm=None, ngram_range=(1, 1))\n",
        "    vec.fit(corpus)\n",
        "\n",
        "    # vectorize the corpus and document\n",
        "    corpus_sparse = vec.transform(corpus)\n",
        "    document_sparse = vec.transform(document)\n",
        "\n",
        "    # create and fit the model\n",
        "    model = KNeighborsClassifier(n_neighbors=5, metric=(lambda x, y: 1 - cosine_similarity(x, y)))\n",
        "    model.fit(corpus_sparse, np.array(classifications))\n",
        "\n",
        "    return model.predict(document_sparse)"
      ],
      "metadata": {
        "id": "Ec63XPL0AjbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yznNQHTxACrW"
      },
      "outputs": [],
      "source": [
        "def prune_tokens(dict, minFreq, maxLen):\n",
        "    tokensToRemove = []\n",
        "\n",
        "    for key in dict.keys():\n",
        "        if (dict[key] <= minFreq or maxLen <= (len(key) and \" \" not in key)):\n",
        "            # add it to a list of tokens to prune\n",
        "            tokensToRemove.append(key)\n",
        "\n",
        "    for token in tokensToRemove:\n",
        "        del dict[token]\n",
        "\n",
        "    return dict\n",
        "\n",
        "def get_spam_scores(documents, corpus):\n",
        "    # add the bias\n",
        "    scores = pd.Series(data=np.zeros(len(documents)))\n",
        "\n",
        "    lenWeight = 0.01\n",
        "\n",
        "    model_predictions = pd.Series(data = knn_predict_spam(documents, corpus).astype(float))\n",
        "    print(model_predictions.head())\n",
        "    scores = scores + model_predictions\n",
        "    print(model_predictions.head())\n",
        "\n",
        "    # REIMPLEMENT TF-IDF SCORING SYSTEM\n",
        "    # use regexes to parse, score, then replace emails, phone numbers, urls with dummy string for later NLP\n",
        "    # then handle common symbol replacements/letter substitutions\n",
        "    # then use stemming and lemmatization to reduce token vector space\n",
        "    # score = (token tf-idf scores from ham) - (token tf-idf scores from spam)\n",
        "\n",
        "    # if (len(sms) < minSusLen):\n",
        "    #     score += lenWeight * len(sms)\n",
        "\n",
        "    return scores\n",
        "\n",
        "def format(stat):\n",
        "    return \"\\t\" + str('%.3f'%stat) + \"\\t\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk.download()\n",
        "\n",
        "categories = {\"ham\": 0, \"spam\": 1}\n",
        "corpus = pd.read_csv(\"dataset\\SMSSpamCollection.txt\", sep=\"\\t\", on_bad_lines='warn')\n",
        "\n",
        "# clean and prepare the dataset\n",
        "corpus.columns = [\"class\", \"text\"]\n",
        "classifications = corpus[\"class\"]\n",
        "corpus[\"class\"] = classifications.map(categories)\n",
        "corpus = corpus.loc[:200]\n",
        "\n",
        "train, test = train_test_split(corpus, test_size=0.2)\n",
        "predictions = pd.Series(data = get_spam_scores(test[\"text\"], train))\n",
        "\n",
        "expected_and_predictions = pd.concat([test[\"class\"], predictions], axis=1, ignore_index=True)\n",
        "\n",
        "print(expected_and_predictions.head())\n",
        "\n",
        "# # print(corpus.head())\n",
        "# sms = \"free money click here\"\n",
        "# print(sms)\n",
        "#\n",
        "# prediction = get_spam_score(pd.Series(data=[sms]), corpus)[0]\n",
        "# if 0 < prediction:\n",
        "#     print(\"The sms is spam\")\n",
        "# else:\n",
        "#     print(\"The sms is ham\")"
      ],
      "metadata": {
        "id": "CpIHybY1AMVh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}